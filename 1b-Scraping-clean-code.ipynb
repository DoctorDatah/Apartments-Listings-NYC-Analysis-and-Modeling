{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Data \n",
    "#### renthop.com\n",
    "https://www.renthop.com/nyc/apartments-for-rent\n",
    "\n",
    "\n",
    "Scraped more pages and cleaned up the code for just scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import requests \n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import clear_output\n",
    "import html5lib\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.renthop.com/search/nyc?max_price=50000&min_price=0&page=2&sort=hopscore&q=&search=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(listing_divs): \n",
    "    listing_list = [] \n",
    "    for idx in range(len(listing_divs)): \n",
    "        indv_listing = [] \n",
    "        current_listing = listing_divs[idx] \n",
    "        href = current_listing.select('a[id*=title]')[0]['href'] \n",
    "        addy = current_listing.select('a[id*=title]')[0].string \n",
    "        hood = current_listing.select('div[id*=hood]')[0].string.replace('\\n','') \n",
    " \n",
    "        indv_listing.append(href) \n",
    "        indv_listing.append(addy) \n",
    "        indv_listing.append(hood) \n",
    " \n",
    "        listing_specs = current_listing.select('table[id*=info] tr') \n",
    "        for spec in listing_specs: \n",
    "            try: \n",
    "                values = spec.text.strip().replace(' ', '_').split() \n",
    "                clean_values = [x for x in values if x != '_'] # Not getting  '_' these values \n",
    "                indv_listing.extend(clean_values) \n",
    "            except: \n",
    "                indv_listing.extend(np.Unknownn) \n",
    "        listing_list.append(indv_listing) \n",
    "    return listing_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_pages(number_pages = 100):\n",
    "    url_prefix = \"https://www.renthop.com/search/nyc?max_price=50000&min_price=0&page=\" \n",
    "    page_no = 1 \n",
    "    url_suffix = \"&sort=hopscore&q=&search=0\" \n",
    "    all_pages_parsed = []\n",
    "\n",
    "    for i in range(number_pages):\n",
    "    \n",
    "        target_page = url_prefix + str(page_no) + url_suffix\n",
    "\n",
    "        # Cleart the ouput and then print new one\n",
    "        print(target_page)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        r = requests.get(target_page)\n",
    "\n",
    "        soup = BeautifulSoup(r.content, 'html5lib')\n",
    "\n",
    "        listing_divs = soup.select('div[class*=search-info]')\n",
    "\n",
    "        one_page_parsed = parse_data(listing_divs)\n",
    "\n",
    "        all_pages_parsed.extend(one_page_parsed)\n",
    "\n",
    "        page_no += 1\n",
    "        \n",
    "    print(\"Completed\")\n",
    "    return all_pages_parsed\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "all_pages_parsed = scrap_pages(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Frame \n",
    "listings  = pd.DataFrame(all_pages_parsed, columns=['url', 'address', 'neighborhood', 'rent', 'beds', 'baths', \"Unknown\"],) \n",
    "listings.replace('None', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have few flex rooms values,  that why some of bath is skewed to next column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing Flex Rooms and Baths issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings[\"Unknown\"], listings[\"baths\"] = np.where(listings[\"Unknown\"].notnull() , \n",
    "                                                  [listings[\"baths\"], listings[\"Unknown\"]], \n",
    "                                                  [listings[\"Unknown\"], listings[\"baths\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming \n",
    "new_columns = listings.columns.values\n",
    "new_columns[6] = 'flexs'\n",
    "listings.columns = new_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir = 'Data'\n",
    "if(os.path.isdir(dir) is False):\n",
    "    os.makedirs(\"Data\")\n",
    "    \n",
    "listings.to_csv(\"Data\\listings_2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
